{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcda7e8b",
   "metadata": {},
   "source": [
    "# SpotMask\n",
    "## A Face Mask Correctness Detection Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e0436",
   "metadata": {},
   "source": [
    "Jacopo Bugini - 207525 - 2020/2021\n",
    "<br>\n",
    "*Deep Learning for Computer Vision (20600)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dcc20",
   "metadata": {},
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b261fe",
   "metadata": {},
   "source": [
    "```\n",
    "1. Introduction\n",
    "   └─ 1.1 Business Idea\n",
    "   └─ 1.2 Problem Definition\n",
    "   \n",
    "2. Methodology\n",
    "   └─ 2.1 Project Structure & Strategy\n",
    "   └─ 2.2 Datasets\n",
    "\n",
    "3. Development\n",
    "   └─ 3.1 Project Repo Structure\n",
    "   └─ 3.2 Project Rquirements\n",
    "   └─ 3.3 Models   \n",
    "          └─ Face Detection    \n",
    "          └─ Mask Detection   \n",
    "          └─ Mask Correctness   \n",
    "          └─ Mask Suggestions  \n",
    "\n",
    "4. Improvements & Challenges\n",
    "\n",
    "5. SpotMask\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc79e6",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53856aae",
   "metadata": {},
   "source": [
    "## 1. Introductuion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ed331",
   "metadata": {},
   "source": [
    "### 1.1 Business Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab31ca",
   "metadata": {},
   "source": [
    "COVID-19 pandemic (as of August 2021) has sickened more than 203 million people globally and claimed the lives of more than 4.2 million people worldwide. With new variants coming out in different places and the vaccination process that is advancing at different rates around the globe the current crisis is still far away to be over.\n",
    "\n",
    "For this reason, now more than ever, ensuring the respect of hygenic standards and law enforcements is extremely important. Especially in public places, on public transports and in situations where people need to stay close to each other or in a restricted spaces.\n",
    "\n",
    "Face mask is now compulsory in many different scenarios according to each country / region / city regulations and it is currently very difficult to monitor and expensive in terms of resources and security agents. An automated facemask correctness detector would be really useful in terms of business usage and resource efficient.\n",
    "\n",
    "Such automatic detector could be placed in different locations where the need of compliance with rules is even more sensible, like airports, public transports, museums and many other places. The deployment costs are limited to the hardware at disposal and at the warning mechanism that the entity would like to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144b484",
   "metadata": {},
   "source": [
    "### 1.2 Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12065724",
   "metadata": {},
   "source": [
    "The problem that we want to address is being able to **determin automatically and quicly if a mask is worn properly or not** and **suggest the user how to wear it correctly** based on how he is keeping it.\n",
    "\n",
    "In order to be able to assess the correctness of a mask we have three major incremental steps (or goals):\n",
    "\n",
    "1. **Detect the face/s** in a specific frame.\n",
    "2. Detecting weather a mask is **worn or not**.\n",
    "3. Detecting weather a mask is **worn correclty or incorrectly**.\n",
    "\n",
    "Eventually, we will try to bring it one step further increasing the complexity of step 3 even more:\n",
    "\n",
    "4. **Classifying why it is not correct** and prompting a suggestion in order to make it properly worn\n",
    "\n",
    "The project is ment to provide an output in real time due to its business purpose and impact, hence the project outcome will be an executable script that through the device' webcam will detect and prompt the results accordingly. <br>*More details will follow in Section 5*.\n",
    "\n",
    "The major concerns regarding the project involve the dataset, given the freshness off the topic there are only limited real datasets available at the moment. Many researches are now focusing on generating artificial datasets able to support such developement and researches. As we will see later we will use also one of these *artificial* dataset in our project. <br>*More details will follow in Section 3 and Section 4*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19644050",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7732ec8",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb9ab5",
   "metadata": {},
   "source": [
    "### 2.1 Project Structure & Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c4fde",
   "metadata": {},
   "source": [
    "The project structure is divided into three main clusters:\n",
    "1. **Face Detection** -> Detect weather one or more face/s is/are present into a specific frame\n",
    "2. **Mask Detection** -> Given a detected face, detect weather the subject has a mask or not\n",
    "3. **Mask Correctness** -> Given a detected subject with a mask, classify weather the mask is properly or improperly worn\n",
    "\n",
    "For each one of the differen steps we are going to define a specific methodology and adopt a specific model according to the task. The specs and details off each model will be found in section 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0670fb0",
   "metadata": {},
   "source": [
    "### 2.2 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf5cb99",
   "metadata": {},
   "source": [
    "- **Flickr-Faces-HQ Dataset (FFHQ)**\n",
    "    \n",
    "    >*[A Style-Based Generator Architecture for Generative Adversarial Networks\n",
    "    Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)\n",
    "    https://github.com/NVlabs/ffhq-dataset]*\n",
    "\n",
    "    Flickr-Faces-HQ Dataset (FFHQ) dataset consist of human faces images which includes considerable variations in terms of age, ethnicity and image background, with also a great coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr and then automatically aligned and cropped.\n",
    "    <br>\n",
    "    The original dataset consists of 70,000 high-quality PNG images at 1024×1024 resolution and contains considerable variation in terms of age, ethnicity and image background. However for our scope we are using the thumbnail version already cropped down to 128x128 pixels, both for size matter and computational power.\n",
    "    \n",
    "    ![image](https://raw.githubusercontent.com/NVlabs/ffhq-dataset/master/ffhq-teaser.png)\n",
    "    \n",
    "    <br>\n",
    "    <br>\n",
    "    \n",
    "- **MaskedFace-Net**\n",
    "\n",
    "    >*[Cabani A, Hammoudi K, Benhabiles H, Melkemi M. MaskedFace-Net - A dataset of correctly/incorrectly masked face images in the context of COVID-19. Smart Health (Amst). 2021;19:100144. doi:10.1016/j.smhl.2020.100144, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7837194/#fn3]*\n",
    "    \n",
    "    A dataset of correctly/incorrectly masked face images in the context of COVID-19. The dataset is generated artificially by the researchers due to the lack of generalized and vast dataset regarding incorrectness and correctness of mask usage.\n",
    "    <br>\n",
    "    The datasets consists of 66,000 high quality images of proper worn masks and 66,000 high quality images of improper worn mask. For the purpose of our project we are going to use a subset of those images for computational and memory constraints.\n",
    "    \n",
    "     ![image](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7837194/bin/gr1_lrg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dd439",
   "metadata": {},
   "source": [
    "**Dataset Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b344f",
   "metadata": {},
   "source": [
    "The datasets used can be found at this drive link already clustered in sets.\n",
    "The dataset structure is reflecting the project structure and strategy defined above (3.1) we will go through each one of the dataset in the following section for the developement phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de27f9",
   "metadata": {},
   "source": [
    "```\n",
    "dataset\n",
    "│   sources.txt    \n",
    "│\n",
    "└─── detect_mask_dataset\n",
    "│    └─── test\n",
    "│    │    └─── mask (10,000)\n",
    "│    │    └─── no_mask (10,000)\n",
    "│    └─── train\n",
    "│    │    └─── mask (30,000)\n",
    "│    │    └─── no_mask (30,000)\n",
    "│    └─── validation\n",
    "│         └─── mask (14,000)\n",
    "│         └─── no_mask (14,000)\n",
    "│   \n",
    "└─── mask_correctness_dataset\n",
    "│    └─── test\n",
    "│    │    └─── proper (9,000)\n",
    "│    │    └─── improper (9,000)\n",
    "│    └─── train\n",
    "│    │    └─── proper (30,000)\n",
    "│    │    └─── improper (30,000)\n",
    "│    └─── validation\n",
    "│    │    └─── proper (4,000)\n",
    "│    │    └─── improper (4,000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538569f6",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990be690",
   "metadata": {},
   "source": [
    "## 3. Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be996f7",
   "metadata": {},
   "source": [
    "### 3.1 Project Repo Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fe75b",
   "metadata": {},
   "source": [
    "The repository is organized as follows:\n",
    "\n",
    "- The main executable file is ```SpotMask.py``` that will be explained and utilized in section 5.\n",
    "- The ```dataset``` folder contains the different dataset used as previously anticipated.\n",
    "- The ```facemask-correctbess-model.ipynb``` is the jupyter containing the training and testing for each model.\n",
    "- The ```utils``` folder contains useful functions and templates used for the report (images, graphs,etc.).\n",
    "- The ```models``` folder contains the different trained models and their variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c0e5d",
   "metadata": {},
   "source": [
    "**Repo's Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24ea5d",
   "metadata": {},
   "source": [
    "```\n",
    "facemask-correctness-detection\n",
    "│   README.md \n",
    "│   Report.ipynb \n",
    "│   LICENSE \n",
    "│   facemask-correctness-model.ipynb \n",
    "│   SpotMask.py \n",
    "│   utils.py \n",
    "│\n",
    "└─── dataset\n",
    "│    └─── detect_mask_dataset\n",
    "│    └─── mask_correctness_dataset\n",
    "│    └─── mask_suggestion_dataset\n",
    "│   \n",
    "└─── models\n",
    "│    └─── face-detection\n",
    "│    │    └─── haar-cascade\n",
    "│    │    └─── yolo-v3\n",
    "│    └─── mask-detection\n",
    "│    │    └─── hyperparameters_tuning\n",
    "│    │    └─── mask_detection.h5\n",
    "│    └─── facemask-correctness\n",
    "│    │    └─── hyperparameters_tuning\n",
    "│    │    └─── facemask_correctness.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0601c09",
   "metadata": {},
   "source": [
    "In order to access the repo here below the cloning snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/JacopoBugini/facemask-correctness-detection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6806c60",
   "metadata": {},
   "source": [
    "**NB** *Please note that the dataset is not present in the repository for storage limits, it can be found at the Google Drive Link shared, or from here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b09925",
   "metadata": {},
   "source": [
    "### 3.2 Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4404d7c",
   "metadata": {},
   "source": [
    "The project is entirely developed in python with the support of:\n",
    "\n",
    "- **Keras** for what concerns the models\n",
    "  > Chollet, F., & others. (2015). Keras. https://keras.io.\n",
    "  \n",
    "- **OpenCV** for what concerns the video interface and image preprocessing\n",
    "  > Bradski, G. (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools.\n",
    "\n",
    "- A built-in **GPU** has been used for the training and testing phases, hence in trying to replicate training timings expect a slight increase if you do not have one on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7fe99",
   "metadata": {},
   "source": [
    "### 3.3 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f0432",
   "metadata": {},
   "source": [
    "Note that the **training** and **testing** for each model has been carried out in the ```facemask-correctness-model.ipynb``` notebook that can be found in the repository or be reached from here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e4b30",
   "metadata": {},
   "source": [
    "|[![Open In Notebook](https://drive.google.com/uc?export=view&id=1xfYuNfDKlhQORXi4_oyeFVoWsXX6zbeV)](https://github.com/JacopoBugini/SpotMask/blob/main/facemask-correctness-model.ipynb)|[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JacopoBugini/SpotMask/blob/main/facemask-correctness-model.ipynb)|\n",
    "|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5095081",
   "metadata": {},
   "source": [
    "**NB** *Note that in order to be executed in colab additional steps are required in order to mount drive and make the dataset available for the training phase*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e5cb5",
   "metadata": {},
   "source": [
    "Let's import load_model from keras in order to display the trained models below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461f388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fca10",
   "metadata": {},
   "source": [
    "### (A) Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0c691",
   "metadata": {},
   "source": [
    "For the face detection problem we needed to identify and capture the different faces present in a determined frame. The two model that we tested are a pre-trained face detection version of **YOLO (You Only Learn Once)** and the more used **Haar Cascade**.\n",
    "\n",
    "After a few tests eventually we opted for **YOLO-v3** as during the testing phase was more stable, fast and precise.\n",
    "\n",
    "The model will be found in ```models/face-detection/yolo-v3``` with the corresponding ```.weights``` and ```.cfg``` files.\n",
    "\n",
    "References for both models:\n",
    "\n",
    "- **YOLO-v3** \n",
    "> Singh S, Ahuja U, Kumar M, Kumar K, Sachdeva M. Face mask detection using YOLOv3 and faster R-CNN models: COVID-19 environment [published online ahead of print, 2021 Mar 1]. Multimed Tools Appl. 2021;1-16. doi:10.1007/s11042-021-10711-8\n",
    "\n",
    "- **Haar Cascade** \n",
    "> https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1d5e9",
   "metadata": {},
   "source": [
    "![Image](https://drive.google.com/uc?export=view&id=1LDqnsionr3SQmcX0AysVeqfWyq5vjxr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43640dfe",
   "metadata": {},
   "source": [
    "### (B) Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4802f9",
   "metadata": {},
   "source": [
    "Mask detection is carried out with a CNN model trained on the ```mask-detection``` dataset:\n",
    "- **mask**, including a variegate set of images with masks both worn correctly and improperly in order to generalize at best the mask detection also when worn in a bad way. The dataset used is the MaskedFace-Net.\n",
    "\n",
    "- **no_mask**, a set of images from the Flickr-Faces-HQ Dataset without any sort of mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda69df",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7b033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 3,453,634\n",
      "Trainable params: 3,453,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mask_detection_model = load_model('models/mask-detection/mask_detection_model.h5')\n",
    "mask_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd023bee",
   "metadata": {},
   "source": [
    "Outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71dc2c",
   "metadata": {},
   "source": [
    "|![Image](https://drive.google.com/uc?export=view&id=14vOeTrRH7ygqqVP6Jncm98vkq5qIfy0J)|![Image](https://drive.google.com/uc?export=view&id=1sN_7jtMAWnd6CvU_u0PuGO9JnFZCvWwf)|\n",
    "|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0bb1d",
   "metadata": {},
   "source": [
    "### (C) Mask Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fdfbc",
   "metadata": {},
   "source": [
    "Mask correctnes is carried out with a sequential CNN model trained on the ```mask-correctness``` dataset:\n",
    "- **proper**, including a set of images with mask worn correctly convering accordingly nose, mouthand chin. The dataset used is the MaskedFace-Ne (*FaceDetectionCorrectMask*).\n",
    "\n",
    "- **improper**, including a variegate set of images with masks worn improperly in order to generalize at best the mask detection also when worn in unusual or strange ways. The dataset used is the MaskedFace-Net (*FaceDetectionImproperMask*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd6b33",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343529e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                156850    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 418,840\n",
      "Trainable params: 418,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mask_correctness_model = load_model('models/facemask-correctness/mask_correctness_model.h5')\n",
    "mask_correctness_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b095105",
   "metadata": {},
   "source": [
    "Outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3454e",
   "metadata": {},
   "source": [
    "|![Image](https://drive.google.com/uc?export=view&id=1GyAKkS5fgHZsmCHzEpsISdmRxwvkzbbP)|![Image](https://drive.google.com/uc?export=view&id=1q_lS9AmhTaxW3ljiAfKYNn9ADZQ-8S3_)|\n",
    "|-|-|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3894a",
   "metadata": {},
   "source": [
    "### (D) Mask Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97137790",
   "metadata": {},
   "source": [
    "Mask suggestions is carried out with a CNN model trained on the ```suggestions``` dataset:\n",
    "- **Proper** - The mask is properly worn\n",
    "- **Off** - The mask is under the chin not covering any part of the face leaving nose and mouth uncovered\n",
    "- **Nose** - The mask is worn leaving the nose outside\n",
    "- **Chion** - The Mask is covering mouth and nose but is worn incorrectly leaving the chin outside\n",
    "\n",
    "This time we will try to generate a model utilizing a hyperparameter tuner in order to find the most efficient model. You can read more in the tuning section in the tarining notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc9f74",
   "metadata": {},
   "source": [
    "More on the hyperparameter tuning through ```keras-tuner``` here:\n",
    "> O'Malley, T., Bursztein, E., Long, J., Chollet, F., Jin, H., Invernizzi, L., & others. (2019). Keras Tuner. https://github.com/keras-team/keras-tuner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd34ef",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9f3400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 3,453,634\n",
      "Trainable params: 3,453,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "suggestuions_model = load_model('models/suggestions-detection/suggestions_model.h5')\n",
    "mask_detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a9def",
   "metadata": {},
   "source": [
    "Outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e8f86",
   "metadata": {},
   "source": [
    "|![Image](https://drive.google.com/uc?export=view&id=1vqlZxV8txfIB191hiqtVIsnimROeLGBg)|![Image](https://drive.google.com/uc?export=view&id=151Ia_JW2YtJ-d-UWUkl0K478R4uIXGE0)|\n",
    "|-|-|\n",
    "|![Image](https://drive.google.com/uc?export=view&id=1130Cds3Hx-SzZ31uFwg6bNy6lX4LDkgf)|![Image](https://drive.google.com/uc?export=view&id=1LeA_bmJLwqz6KGwmaKmSQ3C_sQ6IL-TY)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0cbbcd",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ea6db",
   "metadata": {},
   "source": [
    "## 4. Improvements & Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4360f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439de088",
   "metadata": {},
   "source": [
    "_______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4d9f3",
   "metadata": {},
   "source": [
    "## 5. SpotMask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1971c",
   "metadata": {},
   "source": [
    "The final software is executable through the SpotMask python file. Here below a quick example with a fast guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa89d60",
   "metadata": {},
   "source": [
    "```python SpotMask-py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a6989",
   "metadata": {},
   "source": [
    "The only argument needed by the parser is the ```--mode``` argument which allows to decide weather to execute the software with the suggestions model or the simple correctness model.\n",
    "\n",
    "For the first one you need to execute ```python SpotMask-py --mode 'suggestions'``` while for the second one ```python SpotMask-py --mode 'simple'``` shall be used. \n",
    "\n",
    "```--mode 'suggestions'``` is the default one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3508f076",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38157f85",
   "metadata": {},
   "source": [
    "# Thanks!\n",
    "\n",
    "**Jacopo Bugini** <br>\n",
    "jacopo.bugini@studbocconi.it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
